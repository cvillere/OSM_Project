{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN STREET MAP DATA CASE STUDY\n",
    "\n",
    "MAP AREA:\n",
    "San Francisco, CA, United States\n",
    "\n",
    "https://mapzen.com/data/metro-extracts/metro/san-francisco_california/\n",
    "\n",
    "\n",
    "\n",
    "The reason why I decided to chose San Francisco is I am pretty familiar with the Bay Area. I like the layout of the city, and its the tech hub of the world. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "#Regular Expression created to remove underscores to normalize/clean the data\n",
    "UNDER_SCORE = re.compile(r'([a-zA-Z0-9_])_([a-zA-Z0-9_])', re.IGNORECASE)\n",
    "\n",
    "#Regular Expression created to remove semi colons\n",
    "SEMI_COLON = re.compile(r'([a-zA-Z0-9_ \\t\\n\\r\\f\\v]);([a-zA-Z0-9_ \\t\\n\\r\\f\\v])')\n",
    "\n",
    "SCHEMA = schema.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is a function utilized in the shape element function to remove underscores from the k and v attributes of tag \n",
    "#elements\n",
    "def remove_underscores(input):\n",
    "    v_under_score = UNDER_SCORE.search(input)\n",
    "    if v_under_score:\n",
    "        k_value_replace = input.replace('_',' ')\n",
    "        return k_value_replace\n",
    "    else:\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is a function utilized in the shape element function to remove semi colons from the k and v attributes of tag \n",
    "#elements. \n",
    "\n",
    "def remove_semicolons(input):\n",
    "    v_semi_colon = SEMI_COLON.search(input)\n",
    "    if v_semi_colon:\n",
    "        k_value_split = input.split(\";\")\n",
    "        k_value_final = k_value_split[0]\n",
    "        return k_value_final\n",
    "    else:\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This cell is used to clean the street names for both the node and way elements\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"St\" : \"Street\",\n",
    "            \"St.\" : \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\" : \"Avenue\",\n",
    "            \"st\" : \"Street\",\n",
    "            \"st.\" : \"Street\",\n",
    "            \"Blvd.\" : \"Boulevard\",\n",
    "            \"Blvd\" : \"Boulevard\",\n",
    "            \"street\" : \"Street\",\n",
    "            \"avenue\" : \"Avenue\",\n",
    "            \"parkway\" : \"Parkway\"}\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    \n",
    "    \n",
    "    name_object = street_type_re.search(name)\n",
    "    if name_object:\n",
    "    # YOUR CODE HERE\n",
    "        if name_object.group() in mapping.keys():\n",
    "            name = re.sub(street_type_re, mapping[name_object.group()], name)\n",
    "        \n",
    "            \n",
    "    return name\n",
    "\n",
    "\n",
    "#Below function used to execute the cleaning of the street names\n",
    "def test():\n",
    "    st_types = audit(OSM_FILE)\n",
    "    #print st_types\n",
    "    #assert len(st_types) == 3\n",
    "    #pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        for item in NODE_FIELDS:\n",
    "            try:\n",
    "                node_attribs[item] = element.attrib[item]\n",
    "            except:\n",
    "                node_attribs[item] = \"9999999\"\n",
    "        \n",
    "        for child in element:\n",
    "                \n",
    "            k_problems = PROBLEMCHARS.search(child.attrib['k'])\n",
    "            k_semi_colon = LOWER_COLON.search(child.attrib['k'])\n",
    "            v_problems = PROBLEMCHARS.search(child.attrib['v'])\n",
    "            \n",
    " \n",
    "            \n",
    "            if k_problems or v_problems:\n",
    "                pass\n",
    "            \n",
    "                \n",
    "            elif k_semi_colon:\n",
    "                node_tags_dict = {}\n",
    "                v_value = remove_underscores(child.attrib[\"v\"])\n",
    "                v_value_semicolon = remove_semicolons(v_value)\n",
    "                node_tags_dict[\"value\"] = v_value_semicolon\n",
    "                \n",
    "                node_tags_dict[\"id\"] = node_attribs[\"id\"]\n",
    "                k_value = remove_underscores(child.attrib['k'])\n",
    "                k_value_semicolon = remove_semicolons(k_value)\n",
    "                \n",
    "                k_value_split = k_value_semicolon.split(\":\")\n",
    "                \n",
    "                \n",
    "                if len(k_value_split) <= 2:\n",
    "                    node_tags_dict[\"key\"] = k_value_split[1]\n",
    "                    node_tags_dict[\"type\"] = k_value_split[0]\n",
    "                if len(k_value_split) > 2:\n",
    "                    node_tags_dict[\"key\"] = k_value_split[1] + \":\" + k_value_split[2]\n",
    "                    node_tags_dict[\"type\"] = k_value_split[0]\n",
    "        \n",
    "                tags.append(node_tags_dict)\n",
    "            \n",
    "            else:\n",
    "                node_tags_dict = {}\n",
    "                k_value = remove_underscores(child.attrib['k'])\n",
    "                k_value_semicolon = remove_semicolons(k_value)\n",
    "                node_tags_dict[\"key\"] = k_value_semicolon\n",
    "                \n",
    "                v_value = remove_underscores(child.attrib[\"v\"])\n",
    "                v_value_semicolon = remove_semicolons(v_value)\n",
    "                node_tags_dict[\"value\"] = v_value_semicolon\n",
    "                \n",
    "                node_tags_dict[\"id\"] = node_attribs[\"id\"]\n",
    "                node_tags_dict[\"type\"] = default_tag_type\n",
    "                \n",
    "                tags.append(node_tags_dict)\n",
    "            \n",
    "\n",
    "        #print {'node': node_attribs, 'node_tags': tags}\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for item in WAY_FIELDS:\n",
    "            try:\n",
    "                way_attribs[item] = element.attrib[item]\n",
    "            except:\n",
    "                way_attribs[item] = \"9999999\"\n",
    "                \n",
    "        count = 0\n",
    "        for child in element:\n",
    "            if child.tag == \"nd\":\n",
    "                way_nodes_dict = {}\n",
    "                way_nodes_dict['id'] = way_attribs['id'] \n",
    "                if child.get(\"ref\"):\n",
    "                    way_nodes_dict[\"node_id\"] = child.attrib['ref']\n",
    "                    way_nodes_dict['position'] = count\n",
    "                    count = count + 1\n",
    "                way_nodes.append(way_nodes_dict)\n",
    "            \n",
    "            elif child.tag == \"tag\":\n",
    "                \n",
    "                k_problems = PROBLEMCHARS.search(child.attrib['k'])\n",
    "                k_semi_colon = LOWER_COLON.search(child.attrib['k'])\n",
    "                v_problems = PROBLEMCHARS.search(child.attrib['v'])\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                if k_problems or v_problems:\n",
    "                    pass\n",
    "                    \n",
    "                \n",
    "                \n",
    "                elif k_semi_colon:\n",
    "                    way_tags_dict = {}\n",
    "                    v_value = remove_underscores(child.attrib[\"v\"])\n",
    "                    v_value_semicolon = remove_semicolons(v_value)\n",
    "                    way_tags_dict[\"value\"] = v_value_semicolon\n",
    "                    \n",
    "                    way_tags_dict[\"id\"] = way_attribs[\"id\"]\n",
    "                    k_value = remove_underscores(child.attrib['k'])\n",
    "                    k_value_semicolon = remove_semicolons(k_value)\n",
    "                    \n",
    "                    k_value_split = k_value_semicolon.split(\":\")\n",
    "                \n",
    "                    if len(k_value_split) <= 2:\n",
    "                        way_tags_dict[\"key\"] = k_value_split[1]\n",
    "                        way_tags_dict[\"type\"] = k_value_split[0]\n",
    "                    if len(k_value_split) > 2:\n",
    "                        way_tags_dict[\"key\"] = k_value_split[1] + \":\" + k_value_split[2]\n",
    "                        way_tags_dict[\"type\"] = k_value_split[0]\n",
    "                    \n",
    "                    tags.append(way_tags_dict)\n",
    "                    \n",
    "                else:\n",
    "                    way_tags_dict = {}\n",
    "                    k_value = remove_underscores(child.attrib['k'])\n",
    "                    k_value_semicolon = remove_semicolons(k_value)\n",
    "                    way_tags_dict[\"key\"] = k_value_semicolon\n",
    "                    \n",
    "                    v_value = remove_underscores(child.attrib[\"v\"])\n",
    "                    v_value_semicolon = remove_semicolons(v_value)\n",
    "                    way_tags_dict[\"value\"] = v_value_semicolon\n",
    "                    \n",
    "                    way_tags_dict[\"id\"] = way_attribs[\"id\"]\n",
    "                    way_tags_dict[\"type\"] = default_tag_type\n",
    "\n",
    "                    tags.append(way_tags_dict)\n",
    "                \n",
    "            \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_FILE, validate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEMS ENCOUNTERED IN YOUR MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEMS ENCOUNTERED DURING WRANGLING\n",
    "\n",
    "Prologue: In the samples of the data I observed, the data was rather tidy in raw form. I think one of the reasons might be the San Francisco area has a higher level of computer literacy than most other places in the World. Therefore, there might be a bit more mindfulness when importing data to the OSM project. However, the id columns in the CSV files were not unique, so I do not have a primary key in either the nodes database or the ways database. \n",
    "\n",
    "1) Underscores\n",
    "\n",
    "There seemed to be several entries in various columns with underscores, where those values would typically be two words. Some of the time they were entered as two words with a spaces, and other times as two words separated only by an underscore. I wrote a function to eliminate underscores to make the data more uniform and intuitive for queries. \n",
    "\n",
    "2) Semi colons\n",
    "\n",
    "In handfuls of instances, while auditing the data, value entries had what seemed to be two entries. There would be the value entry followed by a semi-colon with another value, or simply additional information to supplement what was to the left of the semi-colon. Using a regular expression, I eliminated the semi-colon and the values to the right of it to make the data more uniform and clean to enter into the database. \n",
    "\n",
    "\n",
    "3) Street Names\n",
    "\n",
    "During auditing I added onto the existing street names function to clean the values of street keys to further make the data more clean and uniform. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below functions were used to audit data to determine fields to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "#Functions used to audit the data\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "def jb_unique_k_and_v(osmfile, tagtype):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    unique_k_and_v = defaultdict(set)\n",
    "    for i, element in enumerate(get_element(osm_file)):\n",
    "        if element.tag == tagtype:                         \n",
    "            for tag in element.iter(\"tag\"):\n",
    "                    unique_k_and_v[tag.attrib['k']].add(tag.attrib['v'])    \n",
    "    print len(unique_k_and_v.keys())  # this is a check\n",
    "    print \"--------------------\"\n",
    "    return unique_k_and_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# OVERVIEW OF THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Is the OSM XML large enough? \n",
    "    - The file I used was roughly 945k KB.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Are overview statistics of the dataset computed?\n",
    "\n",
    "-size of database file\n",
    "\n",
    "PRAGMA PAGE_SIZE\n",
    "1,024\n",
    "PRAGMA PAGE_COUNT\n",
    "528,429 \n",
    "\n",
    "= 541 MB\n",
    "\n",
    "\n",
    "-number of unique users\n",
    "\n",
    "SELECT count(distinct(users_ways.uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) users_ways;\n",
    "2422 users\n",
    "\n",
    "\n",
    "-number of nodes\n",
    "\n",
    "SELECT count(*) FROM nodes;\n",
    "4,514,170 nodes\n",
    "\n",
    "\n",
    "-number of ways\n",
    "\n",
    "SELECT count(*) FROM ways;\n",
    "520,961 ways\n",
    "\n",
    "\n",
    "\n",
    "-TOP 10 most common values from nodes_tags table\n",
    "\n",
    "SELECT value, count(*) as sum FROM nodes_tags GROUP BY value ORDER BY sum desc LIMIT 10;\n",
    "JOSM|12733\n",
    "crossing|7749\n",
    "CA|6408\n",
    "yes|6113\n",
    "turning circle|5969\n",
    "traffic signals|4913\n",
    "US|4035\n",
    "tree|3746\n",
    "Berkeley|3535\n",
    "no|3422\n",
    "\n",
    "\n",
    "-TOP 10 most common values from way_tags table\n",
    "\n",
    "SELECT value, count(*) as sum FROM ways_tags GROUP BY value ORDER BY sum desc LIMIT 10;\n",
    "yes|421565\n",
    "no|58228\n",
    "A41|44081\n",
    "residential|39431\n",
    "service|17971\n",
    "St|11787\n",
    "footway|11116\n",
    "Bing|10857\n",
    "Ave|10280\n",
    "secondary|8662\n",
    "\n",
    "\n",
    "How many node entries have a latitude that is above the mean latitude?\n",
    "\n",
    "SELECT count(*) as sum FROM nodes, (SELECT avg(nodes.lat) as av FROM nodes) as subq WHERE lat > av;\n",
    "2,620,438 node entries\n",
    "\n",
    "\n",
    "Most common 15 cities in the data?\n",
    "\n",
    "SELECT key, value, count(*) as sum FROM (SELECT key, value FROM nodes_tags UNION ALL SELECT key, value FROM ways_tags) WHERE key = \"city\" GROUP BY value ORDER BY sum desc LIMIT 15;\n",
    "city|Berkeley|5618\n",
    "city|Piedmont|3812\n",
    "city|Richmond|1350\n",
    "city|Oakland|1335\n",
    "city|Burlingame|199\n",
    "city|Albany|186\n",
    "city|Alameda|124\n",
    "city|Hayward|78\n",
    "city|Pacifica|75\n",
    "city|Sausalito|56\n",
    "city|Emeryville|39\n",
    "city|Belmont|34\n",
    "city|Colma|30\n",
    "city|Fremont|18\n",
    "city|Alamo|17\n",
    "\n",
    "15 Least popular cuisine choices?\n",
    "\n",
    "Select key, value, count(*) as sum FROM (SELECT key, value FROM nodes_tags UNION ALL SELECT key, value FROM ways_tags) WHERE key = \"cuisine\" GROUP BY value ORDER BY sum asc LIMIT 15;\n",
    "cuisine|Asian|1\n",
    "cuisine|BBQ|1\n",
    "cuisine|Bakery|1\n",
    "cuisine|Boba|1\n",
    "cuisine|Cafe|1\n",
    "cuisine|California|1\n",
    "cuisine|Chicken and waffles|1\n",
    "cuisine|Comfort Food|1\n",
    "cuisine|Cucina Americana|1\n",
    "cuisine|Deli|1\n",
    "cuisine|El Salvadorean|1\n",
    "cuisine|French and American|1\n",
    "cuisine|French-style cafe|1\n",
    "cuisine|Fruit Smoothies|1\n",
    "cuisine|Hawaiian|1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER IDEAS ABOUT THE DATASETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1) Are ideas for additional improvements included?\n",
    "    - Submission document includes one or more additional suggestions for improving and analyzing the data.\n",
    "\n",
    "    1) County in way_tags\n",
    "    \n",
    "    In the way_tags data, county is referred to as \"county, CA.\" This could lead to confusion down the line with a query.\n",
    "    Also, I noticed this in most if not all of the county value entries I observed, but this was only in the sample of the \n",
    "    data file I was using to audit the data. Those county value entries could be in a different form in other \n",
    "    parts of the file. \n",
    "    \n",
    "    2) phone numbers\n",
    "    \n",
    "    The phone numbers are in all different forms. These could be cleaned to glean additional location based information\n",
    "    from the dataset. However, is the effort in auditing and cleaning the data to get whatever information the phone numbers\n",
    "    might provide worth it, given the zip code values can probably be much more exact and useful. Depending on the research\n",
    "    question, an analyst may want to thoroughly audit and clean the phone numbers. It is simply a matter of whether the \n",
    "    effort is worth it for the project. \n",
    "    \n",
    "\n",
    "2) Are benefits and problems with additional improvements discussed?\n",
    "\n",
    "    The benefit to the exploring the cleaning of these possible problems is it makes the data more uniform, and could \n",
    "    possibly increase the amount of information we could glean from the dataset. However, is the additional effort and time \n",
    "    to do what worth it given what we are looking to find from the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python code for \"Case Study: OpenStreetMap Data\" quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterative Parsing\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "def count_tags(filename):\n",
    "    TagList = []\n",
    "    Tag_Dictionary = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        TagList.append(elem.tag)\n",
    "    for item in TagList:\n",
    "        if item not in Tag_Dictionary:\n",
    "            count = 0\n",
    "            for item_tag in Tag_List:\n",
    "                if item_tag == item:\n",
    "                    count = count + 1\n",
    "                Tag_Dictionary[item] = count\n",
    "    return Tag_Dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tag Types\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k = element.attrib[\"k\"]\n",
    "        if re.search(lower, k):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif re.search(lower_colon, k):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif re.search(problemchars, k):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "        \n",
    "        pass\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exploring Users\n",
    "\n",
    "def get_user(element):\n",
    "    if element.tag == \"node\":\n",
    "        idu = element.attrib[\"uid\"]\n",
    "    elif element.tag == \"way\":\n",
    "        idu = element.attrib[\"uid\"]\n",
    "    elif element.tag == \"relation\":\n",
    "        idu = element.attrib[\"uid\"]\n",
    "    else:\n",
    "        idu = None\n",
    "    \n",
    "    userid = idu\n",
    "    \n",
    "    return userid\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if get_user(element) == None:\n",
    "            pass\n",
    "        elif get_user(element) not in users:\n",
    "            users.add(get_user(element))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Improving Street Names\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    name_object = street_type_re.search(name)\n",
    "    if name_object:\n",
    "        if name_object.group() in mapping.keys():\n",
    "            name = re.sub(street_type_re, mapping[name_object.group()], name)\n",
    "            \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing For Database\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"example.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        for item in element.attrib:\n",
    "            if item in NODE_FIELDS:\n",
    "                node_attribs[item] = element.attrib[item]\n",
    "        \n",
    "        for child in element:\n",
    "            node_tags_dict = {}\n",
    "            node_tags_dict[\"value\"] = child.attrib[\"v\"]\n",
    "            node_tags_dict[\"id\"] = element.attrib[\"id\"]\n",
    "                \n",
    "            k_problems = PROBLEMCHARS.search(child.attrib['k'])\n",
    "            k_semi_colon = LOWER_COLON.search(child.attrib['k'])\n",
    "            if k_problems:\n",
    "                pass\n",
    "                \n",
    "            \n",
    "                \n",
    "            elif k_semi_colon:\n",
    "                k_value_split = child.attrib['k'].split(\":\")\n",
    "                \n",
    "                if len(k_value_split) == 2:\n",
    "                    node_tags_dict[\"key\"] = k_value_split[1]\n",
    "                    node_tags_dict[\"type\"] = k_value_split[0]\n",
    "                if len(k_value_split) == 3:\n",
    "                    node_tags_dict[\"key\"] = k_value_split[1] + \":\" + k_value_split[2]\n",
    "                    node_tags_dict[\"type\"] = k_value_split[0]\n",
    "            else:\n",
    "                node_tags_dict[\"key\"] = child.attrib[\"k\"]\n",
    "                node_tags_dict[\"type\"] = default_tag_type\n",
    "\n",
    "            tags.append(node_tags_dict)    \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for item in element.attrib:\n",
    "            if item in WAY_FIELDS:\n",
    "                way_attribs[item] = element.attrib[item]\n",
    "                \n",
    "        count = 0\n",
    "        for child in element:\n",
    "            if child.tag == \"nd\":\n",
    "                way_nodes_dict = {}\n",
    "                way_nodes_dict['id'] = element.attrib['id'] \n",
    "                if child.get(\"ref\"):\n",
    "                    way_nodes_dict[\"node_id\"] = child.attrib['ref']\n",
    "                    way_nodes_dict['position'] = count\n",
    "                    count = count + 1\n",
    "                way_nodes.append(way_nodes_dict)\n",
    "            \n",
    "            elif child.tag == \"tag\":\n",
    "                way_tags_dict = {}\n",
    "                way_tags_dict[\"value\"] = child.attrib[\"v\"]\n",
    "                way_tags_dict[\"id\"] = element.attrib[\"id\"]\n",
    "                \n",
    "                k_problems = PROBLEMCHARS.search(child.attrib['k'])\n",
    "                k_semi_colon = LOWER_COLON.search(child.attrib['k'])\n",
    "                if k_problems:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "                elif k_semi_colon:\n",
    "                    k_value_split = child.attrib['k'].split(\":\")\n",
    "                \n",
    "                    if len(k_value_split) == 2:\n",
    "                        way_tags_dict[\"key\"] = k_value_split[1]\n",
    "                        way_tags_dict[\"type\"] = k_value_split[0]\n",
    "                    if len(k_value_split) == 3:\n",
    "                        way_tags_dict[\"key\"] = k_value_split[1] + \":\" + k_value_split[2]\n",
    "                        way_tags_dict[\"type\"] = k_value_split[0]\n",
    "                else:\n",
    "                    way_tags_dict[\"key\"] = child.attrib[\"k\"]\n",
    "                    way_tags_dict[\"type\"] = default_tag_type\n",
    "\n",
    "                \n",
    "                tags.append(way_tags_dict)        \n",
    "                \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESOURCES\n",
    "\n",
    "1)\n",
    "\n",
    "https://discussions.udacity.com/t/creating-db-file-from-csv-files-with-non-ascii-unicode-characters/174958/45\n",
    "\n",
    "2) \n",
    "\n",
    "https://discussions.udacity.com/t/project-problem-cant-get-through-validate-element-el-validator/179544/37\n",
    "\n",
    "3)\n",
    "\n",
    "https://docs.python.org/2/library/re.html\n",
    "\n",
    "4)\n",
    "\n",
    "https://discussions.udacity.com/t/preparing-for-database-have-i-located-the-ballpark/188800/20\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
